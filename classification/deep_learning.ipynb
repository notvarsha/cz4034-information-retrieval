{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = TextClassifier.load('en-sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %pip install flair\n",
    "# %pip install transformers torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "# Sample DataFrame preparation\n",
    "all_df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Splitting the dataset\n",
    "train_df, test_df = train_test_split(all_df, test_size=0.2, random_state=42)\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['encoded_labels'] = label_encoder.fit_transform(train_df['sentiment'])\n",
    "test_df['encoded_labels'] = label_encoder.transform(test_df['sentiment'])\n",
    "\n",
    "# Preparing the dataset for PyTorch\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        texts = [str(text) for text in texts if pd.notnull(text) and text != '']\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=512)  # Example: Reduced to 128\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Convert DataFrame to Dataset\n",
    "train_df.dropna(subset=['ProcessedComments'], inplace=True)\n",
    "train_df = train_df[train_df['ProcessedComments'] != '']\n",
    "train_df['ProcessedComments'] = train_df['ProcessedComments'].astype(str)\n",
    "\n",
    "test_df.dropna(subset=['ProcessedComments'], inplace=True)\n",
    "test_df = test_df[test_df['ProcessedComments'] != '']\n",
    "test_df['ProcessedComments'] = test_df['ProcessedComments'].astype(str)\n",
    "\n",
    "# Now proceed to create the dataset instances\n",
    "train_dataset = SentimentDataset(train_df['ProcessedComments'].to_list(), train_df['encoded_labels'].to_list(), tokenizer)\n",
    "test_dataset = SentimentDataset(test_df['ProcessedComments'].to_list(), test_df['encoded_labels'].to_list(), tokenizer)\n",
    "\n",
    "# Initialize the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # fp16=True,  # Enable mixed precision\n",
    "    # gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=1,              \n",
    "    per_device_train_batch_size=4,  \n",
    "    per_device_eval_batch_size=8,   \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "from flair.datasets import TREC_6\n",
    "from flair.embeddings import TransformerDocumentEmbeddings, DocumentPoolEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.data import Sentence\n",
    "from flair.models import TARSClassifier\n",
    "from flair.embeddings import FlairEmbeddings, PooledFlairEmbeddings\n",
    "from flair.data import Dictionary\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "from flair.trainers.language_model_trainer import LanguageModelTrainer, TextCorpus\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay, f1_score, precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "all_df = pd.read_csv('data.csv')\n",
    "\n",
    "all_df=all_df[[\"ProcessedComments\",\"sentiment\"]]\n",
    "all_df.dropna(subset=['ProcessedComments'], inplace=True)\n",
    "\n",
    "all_df['formatted'] = all_df.apply(lambda row: row['ProcessedComments'] + ' __label__' + str(row['sentiment']), axis=1)\n",
    "\n",
    "\n",
    "train, test = train_test_split(all_df, train_size = 0.8, shuffle = True, stratify=all_df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data'  # Folder where you want to save the files\n",
    "\n",
    "# Save files\n",
    "train.to_csv(f'{data_folder}/train.csv', index=False, header=False)\n",
    "test.to_csv(f'{data_folder}/test.csv', index=False, header=False)\n",
    "test.to_csv(f'{data_folder}/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProcessedComments</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>everyone died one person got rich seems like s...</td>\n",
       "      <td>1</td>\n",
       "      <td>everyone died one person got rich seems like s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u attentive sleuth knew something wrong since ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>u attentive sleuth knew something wrong since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>setup brave presearch swash provide liquidity ...</td>\n",
       "      <td>1</td>\n",
       "      <td>setup brave presearch swash provide liquidity ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love tweet week profiting btc</td>\n",
       "      <td>1</td>\n",
       "      <td>love tweet week profiting btc __label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tldr member congress hold trade digital asset ...</td>\n",
       "      <td>1</td>\n",
       "      <td>tldr member congress hold trade digital asset ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8495</th>\n",
       "      <td>vote feg coin buy billion early cheap possible...</td>\n",
       "      <td>0</td>\n",
       "      <td>vote feg coin buy billion early cheap possible...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8496</th>\n",
       "      <td>feeling people talking b bitcoin fee never rea...</td>\n",
       "      <td>-1</td>\n",
       "      <td>feeling people talking b bitcoin fee never rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8497</th>\n",
       "      <td>lmao nailed crowd well knowing enough rambling...</td>\n",
       "      <td>1</td>\n",
       "      <td>lmao nailed crowd well knowing enough rambling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8498</th>\n",
       "      <td>sure diligence assume every project posted pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>sure diligence assume every project posted pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8499</th>\n",
       "      <td>gem stone stone stone hectise token auto buy b...</td>\n",
       "      <td>1</td>\n",
       "      <td>gem stone stone stone hectise token auto buy b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8490 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ProcessedComments  sentiment  \\\n",
       "0     everyone died one person got rich seems like s...          1   \n",
       "1     u attentive sleuth knew something wrong since ...         -1   \n",
       "2     setup brave presearch swash provide liquidity ...          1   \n",
       "3                         love tweet week profiting btc          1   \n",
       "4     tldr member congress hold trade digital asset ...          1   \n",
       "...                                                 ...        ...   \n",
       "8495  vote feg coin buy billion early cheap possible...          0   \n",
       "8496  feeling people talking b bitcoin fee never rea...         -1   \n",
       "8497  lmao nailed crowd well knowing enough rambling...          1   \n",
       "8498  sure diligence assume every project posted pro...          1   \n",
       "8499  gem stone stone stone hectise token auto buy b...          1   \n",
       "\n",
       "                                              formatted  \n",
       "0     everyone died one person got rich seems like s...  \n",
       "1     u attentive sleuth knew something wrong since ...  \n",
       "2     setup brave presearch swash provide liquidity ...  \n",
       "3              love tweet week profiting btc __label__1  \n",
       "4     tldr member congress hold trade digital asset ...  \n",
       "...                                                 ...  \n",
       "8495  vote feg coin buy billion early cheap possible...  \n",
       "8496  feeling people talking b bitcoin fee never rea...  \n",
       "8497  lmao nailed crowd well knowing enough rambling...  \n",
       "8498  sure diligence assume every project posted pro...  \n",
       "8499  gem stone stone stone hectise token auto buy b...  \n",
       "\n",
       "[8490 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-08 01:36:48,033 Reading data from data\n",
      "2024-04-08 01:36:48,034 Train: data/train.csv\n",
      "2024-04-08 01:36:48,034 Dev: data/dev.csv\n",
      "2024-04-08 01:36:48,034 Test: data/test.csv\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No data provided when initializing corpus object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/caleb/Desktop/cz4034-information-retrieval/classification/deep_learning.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/caleb/Desktop/cz4034-information-retrieval/classification/deep_learning.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m data_folder \u001b[39m=\u001b[39m Path(\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/caleb/Desktop/cz4034-information-retrieval/classification/deep_learning.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Assuming the formatted data has two columns: the text and '__label__' prefixed label\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/caleb/Desktop/cz4034-information-retrieval/classification/deep_learning.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m corpus: Corpus \u001b[39m=\u001b[39m CSVClassificationCorpus(data_folder,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caleb/Desktop/cz4034-information-retrieval/classification/deep_learning.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                          column_name_map\u001b[39m=\u001b[39m{\u001b[39m0\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mProcessedComments\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caleb/Desktop/cz4034-information-retrieval/classification/deep_learning.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                          train_file\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain.csv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caleb/Desktop/cz4034-information-retrieval/classification/deep_learning.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                                          test_file\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest.csv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caleb/Desktop/cz4034-information-retrieval/classification/deep_learning.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                                          dev_file\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdev.csv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caleb/Desktop/cz4034-information-retrieval/classification/deep_learning.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                                          label_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# If you don't have a dev set, Flair will split the train set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caleb/Desktop/cz4034-information-retrieval/classification/deep_learning.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Check what the corpus looks like\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/caleb/Desktop/cz4034-information-retrieval/classification/deep_learning.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(corpus)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/flair/datasets/document_classification.py:398\u001b[0m, in \u001b[0;36mCSVClassificationCorpus.__init__\u001b[0;34m(self, data_folder, column_name_map, label_type, name, train_file, test_file, dev_file, max_tokens_per_doc, max_chars_per_doc, tokenizer, in_memory, skip_header, encoding, no_class_label, sample_missing_splits, **fmtparams)\u001b[0m\n\u001b[1;32m    362\u001b[0m test \u001b[39m=\u001b[39m (\n\u001b[1;32m    363\u001b[0m     CSVClassificationDataset(\n\u001b[1;32m    364\u001b[0m         test_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    380\u001b[0m dev \u001b[39m=\u001b[39m (\n\u001b[1;32m    381\u001b[0m     CSVClassificationDataset(\n\u001b[1;32m    382\u001b[0m         dev_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    396\u001b[0m )\n\u001b[0;32m--> 398\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(train, dev, test, name\u001b[39m=\u001b[39mname, sample_missing_splits\u001b[39m=\u001b[39msample_missing_splits)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/flair/data.py:1245\u001b[0m, in \u001b[0;36mCorpus.__init__\u001b[0;34m(self, train, dev, test, name, sample_missing_splits)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[39m# abort if no data is provided\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m train \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m dev \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m test:\n\u001b[0;32m-> 1245\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo data provided when initializing corpus object.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1247\u001b[0m \u001b[39m# sample test data from train if none is provided\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m \u001b[39mif\u001b[39;00m test \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m sample_missing_splits \u001b[39mand\u001b[39;00m train \u001b[39mand\u001b[39;00m sample_missing_splits \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39monly_dev\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No data provided when initializing corpus object."
     ]
    }
   ],
   "source": [
    "from flair.datasets import CSVClassificationCorpus\n",
    "from flair.data import Corpus\n",
    "from pathlib import Path\n",
    "\n",
    "# Directory where the train, test, and dev CSV files are located\n",
    "data_folder = Path('data')\n",
    "\n",
    "# Assuming the formatted data has two columns: the text and '__label__' prefixed label\n",
    "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
    "                                         column_name_map={0: \"ProcessedComments\", 1: \"sentiment\"},\n",
    "                                         train_file='train.csv',\n",
    "                                         test_file='test.csv',\n",
    "                                         dev_file='dev.csv',\n",
    "                                         label_type='sentiment')  # If you don't have a dev set, Flair will split the train set\n",
    "\n",
    "# Check what the corpus looks like\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import FlairEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "# Use Flair's pre-trained embeddings\n",
    "word_embeddings = [FlairEmbeddings('news-forward'), FlairEmbeddings('news-backward')]\n",
    "\n",
    "# Initialize document embeddings (consider using DocumentPoolEmbeddings or TransformerDocumentEmbeddings instead)\n",
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                            hidden_size=512,\n",
    "                                            reproject_words=True,\n",
    "                                            reproject_words_dimension=256)\n",
    "from flair.models import TextClassifier\n",
    "\n",
    "# Initialize the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
    "\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "# Initialize the model trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# Start the training process\n",
    "trainer.train('models/classification',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=10)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
